{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2oP1uun77cIh"
   },
   "source": [
    "This project will involve the creation of a spellchecking system and an evaluation of its performance.\n",
    "Please start by downloading the corpus `holbrook.txt` from Data.\n",
    "\n",
    "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
    "\n",
    "    My siter|sister go|goes to Tonbury .\n",
    "    \n",
    "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
    "\n",
    "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
    "\n",
    "    My Mum goes out some_times|sometimes .\n",
    "    \n",
    "You do not need to separate these words, but instead you may treat them like a single token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIVCSJV-7kDs"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
    "\n",
    "Then split your data into a test set of 100 lines and a training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: \n",
    "* For the course of this assignment, all the calculations are done by converting words into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RznCZ1mw7mfk"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "lines = open(\"data/holbrook.txt\").readlines()\n",
    "data = []\n",
    "\n",
    "for line in lines:\n",
    "    words=word_tokenize(line)\n",
    "    #preparing dictionary with original and corrected sentences. \n",
    "    dict={}\n",
    "    dict['original']=[w[0:w.index('|')] if '|' in w else w for w in words ]\n",
    "    dict['corrected']=[w[w.index('|')+1:len(w)] if '|' in w else w for w in words ]\n",
    "    dict['indexes']=[words.index(w) for w in words if '|' in w]\n",
    "    data.append(dict)\n",
    "\n",
    "assert(data[2] == {\n",
    "    'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
    "    'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
    "    'indexes': [9]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRSX4I0H7pSC"
   },
   "source": [
    "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kt9aR2Gy7p1C"
   },
   "outputs": [],
   "source": [
    "#splitting data into train and test data sets.\n",
    "test = data[:100]\n",
    "train = data[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hm5JL7cH7sLK"
   },
   "source": [
    "## **Task 2**\n",
    "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
    "\n",
    "*Hint: use `Counter` to implement this so it may be called many times*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ge0uHS-7uEK"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "#Global variable for storing all words in corrected train data sets.\n",
    "all_words=[]\n",
    "\n",
    "#creating all words from corrected train data set\n",
    "def create_all_words(data_set):\n",
    "    words=list(itertools.chain.from_iterable([d['corrected'] for d in data_set]))\n",
    "    [all_words.append(w.lower()) for w in words]\n",
    "\n",
    "#find frequency of a given word\n",
    "def unigram(word):\n",
    "    return Counter(all_words)[word.lower()]      \n",
    "\n",
    "#find unigram probability of a given word\n",
    "def prob(word):\n",
    "    words_with_frequency=Counter(all_words)\n",
    "    return words_with_frequency[word]/sum([words_with_frequency[freq] for freq in words_with_frequency])\n",
    "\n",
    "create_all_words(train)\n",
    "assert(unigram(\"me\")==87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w8r8QYj78GPK"
   },
   "source": [
    "## **Task 3**: \n",
    "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SV9Mu8P38IQE",
    "outputId": "9f29e22b-0f8b-4b92-9d5f-fcde3efec970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Edit distance returns the number of changes to transform one word to another\n",
    "print(edit_distance(\"there\", \"their\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hm46Lbiz8K8M"
   },
   "source": [
    "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
    "\n",
    "1. Collect the set of all unique tokens in `train`\n",
    "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
    "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
    "\n",
    "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoilAmFW8PCb"
   },
   "outputs": [],
   "source": [
    "def get_candidates(token):\n",
    "    unique_tokens=set(all_words)\n",
    "    dist={}\n",
    "    for word in unique_tokens:\n",
    "        measure=edit_distance(token.lower(),word)\n",
    "        dist[word]=measure\n",
    "    \n",
    "    #returning words with a min edit distance value\n",
    "    return [k for k,v in dist.items() if v==min(dist.values())]\n",
    "\n",
    "#Test your code as follows\n",
    "assert get_candidates(\"minde\") == ['mine', 'mind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGY-eCkN8TIM"
   },
   "source": [
    "## Task 4:\n",
    "\n",
    "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
    "\n",
    "*Your solution to this should involve `get_candidates`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIGKE4_P8WGP"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def correct(sentence):\n",
    "    \n",
    "    #making copy of a original sentence\n",
    "    new_sentence=deepcopy(sentence)\n",
    "    \n",
    "    i=0\n",
    "    while i<len(new_sentence):\n",
    "        #dict to store unigram probabilities of matching words found by edit distance\n",
    "        unigram_probabilities={}\n",
    "        \n",
    "        if new_sentence[i].lower() not in set([w.lower() for w in all_words]):\n",
    "            \n",
    "            #get similar candidates for each of the word\n",
    "            similar_words =get_candidates(new_sentence[i])\n",
    "            \n",
    "            #iterate and find unigram probability of each similar word\n",
    "            for similar_word in similar_words:\n",
    "                unigram_probabilities[similar_word]=prob(similar_word)\n",
    "                \n",
    "            #get correct word by finding max of all unigram probabilities\n",
    "            correct_word=max(unigram_probabilities, key=unigram_probabilities.get)\n",
    "            \n",
    "            #update sentence\n",
    "            new_sentence[i]=correct_word\n",
    "            \n",
    "        i=i+1\n",
    "        \n",
    "    return new_sentence\n",
    "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oG7jC6au8kka"
   },
   "source": [
    "## **Task 5**: \n",
    "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HSXTQypR8mdR",
    "outputId": "853813e4-d71b-42a7-8e96-68d038457628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.0669014084507\n"
     ]
    }
   ],
   "source": [
    "def accuracy(test):\n",
    "    correct_word_count=0\n",
    "    total_word_count=0\n",
    "    \n",
    "    #looping over test data set to find the corrected sentence\n",
    "    for sent in test:\n",
    "        \n",
    "        #calculate total words in corrected data data set\n",
    "        total_word_count=total_word_count+len(sent['original'])\n",
    "        \n",
    "        #get the corrected sentence\n",
    "        result=correct(sent['original'])\n",
    "        \n",
    "        #get correct word count\n",
    "        correct_word_count=correct_word_count+get_correct_word_count(sent['corrected'],result)\n",
    "    #finding accuracy\n",
    "    return (correct_word_count/total_word_count)*100\n",
    "\n",
    "#function to find no of correct words from test data set\n",
    "def get_correct_word_count(test_data_sent,corrected_sent):\n",
    "    i=0\n",
    "    correct_word_count=0\n",
    "    while i<len(test_data_sent):\n",
    "        if test_data_sent[i].lower()==corrected_sent[i].lower():\n",
    "            correct_word_count=correct_word_count+1\n",
    "        i=i+1\n",
    "    return correct_word_count\n",
    "\n",
    "print(\"Accuracy: \"+str(accuracy(test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9b-r2JzD8_Zh"
   },
   "source": [
    "## **Task 6:**\n",
    "\n",
    "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
    "\n",
    "* You may resources beyond those provided here.\n",
    "* You must **not use the test data** in this task.\n",
    "* Provide a short text describing what you intend to do and why. \n",
    "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
    "* Your implementation should not consist of more than 50 lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "\n",
    "* nb_prob() finds probability of a word using Noisy Channel Model.\n",
    "        p(C|W)=>p(W|C)*p(C)\n",
    "* Applying Laplace smoothing to the assign probability.\n",
    "* get_candidates_updated() normalises edit_distance by sum of the length of both the words. Normalizing the edit distance is one way to put the differences between strings on a single scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates_updated(token):\n",
    "    unique_tokens=set(all_words)\n",
    "    dist={}\n",
    "    for word in unique_tokens:\n",
    "        measure=edit_distance(token.lower(),word)\n",
    "        #normalising before assinging it to dict\n",
    "        dist[word]=measure/(len(token.lower())+len(word))\n",
    "    \n",
    "    #returning words with a min edit distance value\n",
    "    return [k for k,v in dist.items() if v==min(dist.values())]\n",
    "\n",
    "#Applying probability according to the Noisy Channel Model\n",
    "#Probability of a similar word(c) given existing word(w)\n",
    "def nb_prob(c,w):\n",
    "    n=0\n",
    "    d=0\n",
    "    unique_tokens=set(all_words)\n",
    "    for data in train:\n",
    "        i=0\n",
    "        while i<len(data['corrected']):\n",
    "            if data['corrected'][i].lower()==c.lower():\n",
    "               d=d+1\n",
    "               if data['original'][i].lower()==w.lower():\n",
    "                   n=n+1\n",
    "            i=i+1\n",
    "    #applying smoothing with alpha taken as 0.5\n",
    "    return (n+0.5/(d+0.5*(len(unique_tokens))))*prob(c)\n",
    "\n",
    "def correct_updated(sentence):\n",
    "    \n",
    "    #making copy of a original sentence\n",
    "    new_sentence=deepcopy(sentence)\n",
    "    \n",
    "    i=0\n",
    "    while i<len(new_sentence):\n",
    "        #dict to store probabilities of matching words found by edit distance\n",
    "        nb_probabilities={}\n",
    "        \n",
    "        if new_sentence[i].lower() not in set([w.lower() for w in all_words]):\n",
    "            \n",
    "            #get similar candidates for each of the word\n",
    "            similar_words =get_candidates_updated(new_sentence[i])\n",
    "            \n",
    "            if len(similar_words)>0:\n",
    "                #iterate and find probability of a given word acc to noisy channel model\n",
    "                for similar_word in similar_words:\n",
    "                    nb_probabilities[similar_word]=nb_prob(similar_word,new_sentence[i])\n",
    "\n",
    "                #get correct word by finding max of all probabilities\n",
    "                correct_word=max(nb_probabilities, key=nb_probabilities.get)\n",
    "\n",
    "                #update sentence\n",
    "                new_sentence[i]=correct_word\n",
    "            \n",
    "        i=i+1\n",
    "        \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLzaC6D28sK9"
   },
   "source": [
    "## **Task 7:**\n",
    "\n",
    "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hw6PzwWn7iEo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.47535211267606\n"
     ]
    }
   ],
   "source": [
    "def accuracy(test):\n",
    "    correct_word_count=0\n",
    "    total_word_count=0\n",
    "    \n",
    "    #looping over test data set to find the corrected sentence\n",
    "    for sent in test:\n",
    "        \n",
    "        #calculate total words in corrected data data set\n",
    "        total_word_count=total_word_count+len(sent['original'])\n",
    "        \n",
    "        #get the corrected sentence\n",
    "        result=correct_updated(sent['original'])\n",
    "        \n",
    "        #get correct word count\n",
    "        correct_word_count=correct_word_count+get_correct_word_count(sent['corrected'],result)\n",
    "    #finding accuracy\n",
    "    return (correct_word_count/total_word_count)*100\n",
    "\n",
    "print(\"Accuracy: \"+str(accuracy(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conlcusion:\n",
    "\n",
    "As clearly seen from the observation, accuracy of a modified algorithm is greater than the existing one."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP 18/19 Assignment 1",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
